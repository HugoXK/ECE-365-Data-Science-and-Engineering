{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d264e6f2",
   "metadata": {},
   "source": [
    "# Part 2, Lab #2: Basic Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047563cc",
   "metadata": {},
   "source": [
    "Ke Xu 3190110360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904db6c",
   "metadata": {},
   "source": [
    "### Due April 6th, 2023 11:59 PM CST\n",
    "#### Logistics and Lab Submission\n",
    "See the **BlackBoard**.\n",
    "#### What You Will Need To Know For This Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c4771",
   "metadata": {},
   "source": [
    "This lab covers:\n",
    "\n",
    "- Design the dataset.\n",
    "- Design the convolutional neural network for the image classification.\n",
    "- Training and testing the model.\n",
    "\n",
    "The submission procedure is provided below:\n",
    "- You will be provided with a Jupyter Notebook for this lab where you need to implement the provided functions as needed for each question. Follow the instructions provided in this Jupyter Notebook (.ipynb) to implement the required functions. \n",
    "- Upload the **PDF** (screen shot) file of your Jupyter Notebook (.ipynb file).\n",
    "- Your grades and feedbacks will appear on BlackBoard. You will have a chance to re-submit your code, only if you have *reasonable* submissions before the deadline (i.e. not an empty script)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c31d56",
   "metadata": {},
   "source": [
    "# Problem 1 : CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5124d",
   "metadata": {},
   "source": [
    "1. Design the first convolutional layers. The first convolutional layer has 16 filters of size 3x3, padding=1, stride=1, please print the size of output. Please refer from [nn.Conv2d()](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534b6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 600, 800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Input 2 images\n",
    "images = torch.randn(2, 3, 600, 800) # batch size, channel, height, weight\n",
    "\n",
    "conv1 = nn.Conv2d(3,16,(3,3),padding=1,stride=1) # input channel, fileters, kernel size, padding, stride\n",
    "output1 = conv1(images)\n",
    "print(output1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fba15b",
   "metadata": {},
   "source": [
    "2. Design the second convolutional layers. Please use the **group convolution**. The group number is 4. The second convolutional layer has 32 filters of size 3x3, padding=1, stride=2, please print the size of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2803c822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 300, 400])\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv2d(16,32,(3,3),padding=1,stride=2,groups=4) # input channel, fileters, kernel size, padding, stride, groups\n",
    "output2 = conv2(output1)\n",
    "print(output2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f4724",
   "metadata": {},
   "source": [
    "3. Design the third convolutional layers. Please use the **dilation**. The third convolutional layer has 16 filters of size 3x3, padding=1, stride=1, dilation=(4,2), please print the size of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f060bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 294, 398])\n"
     ]
    }
   ],
   "source": [
    "conv3 = nn.Conv2d(32,16,(3,3),padding=1,stride=1,dilation=(4,2)) # input channel, fileters, kernel size, padding, stride\n",
    "output3 = conv3(output2)\n",
    "print(output3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2bbc07",
   "metadata": {},
   "source": [
    "# Problem 2 : RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ce72",
   "metadata": {},
   "source": [
    "1. Create RNN with input size 10, hidden size 20, please print the size of output. Please refer from [nn.RNN()](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ee9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 20])\n",
      "torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "# Create input sequence of length 3, with batch size 1\n",
    "x = torch.randn(3, 1, 10)\n",
    "\n",
    "# Initialize hidden state with batch size 1\n",
    "hidden = torch.zeros(1, 1, 20)\n",
    "\n",
    "# Initialized RNN\n",
    "rnn = nn.RNN(10,20) # input size, hidden size\n",
    "output, hidden = rnn(x, hidden)\n",
    "print(output.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347107ec",
   "metadata": {},
   "source": [
    "2. Initialize hidden state with batch size 1, hidden size is 20, please use [torch.zeros()](https://pytorch.org/docs/stable/generated/torch.zeros.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fe9412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1,1,20) # input size, batch size, hidden size\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d31c90",
   "metadata": {},
   "source": [
    "3. Design a fully connected layer to output the final result, the output size is 5. Please refer from [nn.Linear()](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a344d0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(20,5) # input size, output size\n",
    "output = fc(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a69ba7",
   "metadata": {},
   "source": [
    "# Problem 3 : LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003224c",
   "metadata": {},
   "source": [
    "1. Create the LSTM model with an input size of 10, a hidden size of 20, and one LSTM layer. Please refer from [nn.LSTM()](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d8edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(10, 20)\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "# Create the LSTM model\n",
    "lstm = nn.LSTM(10,20) # input size, hidden size\n",
    "print(lstm)\n",
    "\n",
    "# Define a sample input sequence of length 5\n",
    "input_seq = torch.randn(5, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc791b",
   "metadata": {},
   "source": [
    "2. Initialize the hidden state and cell state by using [torch.randn()](https://pytorch.org/docs/stable/generated/torch.randn.html). Print the output shape and final hidden state and cell state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105bbdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 1, 20])\n",
      "Final hidden state: torch.Size([1, 1, 20])\n",
      "Final cell state: torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the hidden state and cell state, size = (num_layers, batch_size, hidden_size)\n",
    "h0 = torch.randn(num_layers, 1, hidden_size)\n",
    "c0 = torch.randn(num_layers, 1, hidden_size)\n",
    "\n",
    "# Pass the input sequence and initial states through the LSTM\n",
    "output, (hn, cn) = lstm(input_seq, (h0, c0)) # input, (h0,c0)\n",
    "\n",
    "# Print the output shape and final hidden state and cell state\n",
    "print('Output shape:', output.shape)\n",
    "print('Final hidden state:', hn.shape)\n",
    "print('Final cell state:', cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bea1c",
   "metadata": {},
   "source": [
    "# Problem 4 : Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48849634",
   "metadata": {},
   "source": [
    "1. Define the input and output embeddings, please refer from [nn.Embedding()](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78a894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128])\n",
      "torch.Size([2, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define the input sequence and target sequence\n",
    "input_seq = torch.tensor([[0, 1, 2, 3]])\n",
    "tgt_seq = torch.tensor([[2, 3, 1, 0], [1,2,3,0]])\n",
    "\n",
    "# Define input and output embeddings, embedding size is 128\n",
    "embedding = nn.Embedding(4,128)  # embedding dim, embedding size\n",
    "\n",
    "src = embedding(input_seq)\n",
    "tgt = embedding(tgt_seq)\n",
    "\n",
    "print(src.shape)\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84e06b",
   "metadata": {},
   "source": [
    "2. Define encoder layer, num_layer=1, the number of heads in the multiheadattention models is 4, the dimension of the hidden layer is 512. Please refer from [nn.TransformerEncoder()](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder) and [nn.TransformerEncoderLayer()](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdde1360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define encoder layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=128,nhead=4) # dim of input,num of heads in the multiheadattention models\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=1) # encoder layer, num of layers\n",
    "encoded = transformer_encoder(src)\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f037eb5",
   "metadata": {},
   "source": [
    "3. Define decoder layer, num_layer=1, the number of heads in the multiheadattention models is 4, the dimension of the hidden layer is 512. Please refer from [nn.TransformerDecoder()](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer) and [nn.TransformerDecoderLayer()](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b53ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define encoder layer\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=128,nhead=4) # dim of input,num of heads in the multiheadattention models\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer,num_layers=1) # encoder layer, num of layers\n",
    "decoded = transformer_decoder(tgt, encoded)\n",
    "print(decoded.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
